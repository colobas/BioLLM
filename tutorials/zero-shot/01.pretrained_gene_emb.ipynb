{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# pretrained gene embedding, without expresssion\n",
    "## 1. scbert \n",
    "1. read the config file(.toml)\n",
    "2. init the LoadScbert object\n",
    "3. run the get_embedding() func.\n",
    "4. evaluate the embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9a0926ea6be20b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from biollm.model.grn import GeneEmbedding\n",
    "from biollm.evaluate.bm_metrices_grn import evaluate, get_genesets_dict\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = '../../case/data/gene_setinfo.pkl'\n",
    "gene_pw_dict = get_genesets_dict(file_path)\n",
    "def evalue_gene_emb(emb, gene_pw_dict):\n",
    "    grn_embed = GeneEmbedding(emb)\n",
    "    result = defaultdict(list)\n",
    "    for i in range(1, 9):\n",
    "        threshold = 0.1 * i\n",
    "        g = grn_embed.generate_network(threshold=threshold)\n",
    "        res = evaluate(g, gene_pw_dict)\n",
    "        result['threshold'].append(threshold)\n",
    "        result['Modularity'].append(res['Modularity'])\n",
    "        result['Biological_index'].append(res['Biological Index'])\n",
    "    return pd.DataFrame(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T02:32:35.561797Z",
     "start_time": "2024-05-16T02:32:35.037301Z"
    }
   },
   "id": "d74bf55a0fde51e0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params token_emb.weight with shape torch.Size([7, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params pos_emb.emb.weight with shape torch.Size([16907, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.calls_since_last_redraw with shape torch.Size([])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.0.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.0.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.0.fn.fast_attention.projection_matrix with shape torch.Size([266, 64])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.0.fn.to_q.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.0.fn.to_k.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.0.fn.to_v.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.0.fn.to_out.weight with shape torch.Size([200, 640])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.0.fn.to_out.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.1.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.1.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.1.fn.fn.w1.weight with shape torch.Size([800, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.1.fn.fn.w1.bias with shape torch.Size([800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.1.fn.fn.w2.weight with shape torch.Size([200, 800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.0.1.fn.fn.w2.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.0.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.0.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.0.fn.fast_attention.projection_matrix with shape torch.Size([266, 64])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.0.fn.to_q.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.0.fn.to_k.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.0.fn.to_v.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.0.fn.to_out.weight with shape torch.Size([200, 640])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.0.fn.to_out.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.1.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.1.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.1.fn.fn.w1.weight with shape torch.Size([800, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.1.fn.fn.w1.bias with shape torch.Size([800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.1.fn.fn.w2.weight with shape torch.Size([200, 800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.1.1.fn.fn.w2.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.0.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.0.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.0.fn.fast_attention.projection_matrix with shape torch.Size([266, 64])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.0.fn.to_q.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.0.fn.to_k.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.0.fn.to_v.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.0.fn.to_out.weight with shape torch.Size([200, 640])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.0.fn.to_out.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.1.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.1.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.1.fn.fn.w1.weight with shape torch.Size([800, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.1.fn.fn.w1.bias with shape torch.Size([800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.1.fn.fn.w2.weight with shape torch.Size([200, 800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.2.1.fn.fn.w2.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.0.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.0.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.0.fn.fast_attention.projection_matrix with shape torch.Size([266, 64])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.0.fn.to_q.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.0.fn.to_k.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.0.fn.to_v.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.0.fn.to_out.weight with shape torch.Size([200, 640])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.0.fn.to_out.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.1.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.1.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.1.fn.fn.w1.weight with shape torch.Size([800, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.1.fn.fn.w1.bias with shape torch.Size([800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.1.fn.fn.w2.weight with shape torch.Size([200, 800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.3.1.fn.fn.w2.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.0.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.0.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.0.fn.fast_attention.projection_matrix with shape torch.Size([266, 64])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.0.fn.to_q.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.0.fn.to_k.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.0.fn.to_v.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.0.fn.to_out.weight with shape torch.Size([200, 640])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.0.fn.to_out.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.1.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.1.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.1.fn.fn.w1.weight with shape torch.Size([800, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.1.fn.fn.w1.bias with shape torch.Size([800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.1.fn.fn.w2.weight with shape torch.Size([200, 800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.4.1.fn.fn.w2.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.0.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.0.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.0.fn.fast_attention.projection_matrix with shape torch.Size([266, 64])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.0.fn.to_q.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.0.fn.to_k.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.0.fn.to_v.weight with shape torch.Size([640, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.0.fn.to_out.weight with shape torch.Size([200, 640])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.0.fn.to_out.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.1.norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.1.norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.1.fn.fn.w1.weight with shape torch.Size([800, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.1.fn.fn.w1.bias with shape torch.Size([800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.1.fn.fn.w2.weight with shape torch.Size([200, 800])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params performer.net.layers.5.1.fn.fn.w2.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params norm.weight with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params norm.bias with shape torch.Size([200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params to_out.weight with shape torch.Size([7, 200])\n",
      "2024-07-29 16:55:53-load_llm[line-50]-INFO: Loading params to_out.bias with shape torch.Size([7])\n",
      "2024-07-29 16:55:53-load_scbert[line-51]-INFO: start to get gene embedding!\n",
      "2024-07-29 16:55:53-load_scbert[line-53]-INFO: start to get gene embedding!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Munch({'model_used': 'scbert', 'emb_type': 'gene', 'model_file': '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/scBERT/ckpt/panglao_pretrain.pth', 'vocab_file': '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/data/train_data/gene2vec/gene_vocab.json', 'model_type': 'scbert', 'output_dir': '../../case/result/zero-shot/', 'device': 'cuda:2', 'bin_num': 7, 'embsize': 200, 'distributed': False, 'batch_size': 8, 'max_seq_len': 16907, 'use_g2v': True, 'g2v_file': '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/data/train_data/gene2vec/gene2vec_16906.npy', 'weight_bias_track': False, 'project_name': 'biollm', 'exp_name': 'scbert_gene_emb_gpu'})\n",
      "gpu used:  0\n",
      "embedding shape: (16906, 200)\n"
     ]
    }
   ],
   "source": [
    "from biollm.utils.utils import load_config\n",
    "import numpy as np\n",
    "from biollm.base.load_scbert import LoadScbert\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "\n",
    "config_file = './configs/scbert_gene_emb.toml'\n",
    "configs = load_config(config_file)\n",
    "\n",
    "obj = LoadScbert(configs)\n",
    "print(obj.args)\n",
    "\n",
    "gene_ids = list(obj.get_gene2idx().values())\n",
    "gene_ids = np.array(gene_ids)\n",
    "gene_ids = torch.tensor(gene_ids, dtype=torch.long).to(configs.device)\n",
    "obj.model = obj.model.to(configs.device)\n",
    "emb = obj.get_embedding(obj.args.emb_type, gene_ids=gene_ids)\n",
    "print('embedding shape:', emb.shape)\n",
    "if not os.path.exists(configs.output_dir):\n",
    "    os.makedirs(configs.output_dir, exist_ok=True)\n",
    "with open(obj.args.output_dir + f'/scbert_{obj.args.emb_type}_emb.pk', 'wb') as w:\n",
    "    res = {'gene_names': list(obj.get_gene2idx().keys()), 'gene_emb': emb}\n",
    "    pkl.dump(res, w)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T08:55:53.406781Z",
     "start_time": "2024-07-29T08:55:52.301594Z"
    }
   },
   "id": "51af038ac663234b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "152.0"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.memory_cached('cuda:2') + torch.cuda.memory_reserved('cuda:2')) / (1024**2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T01:30:55.108201Z",
     "start_time": "2024-06-19T01:30:55.101787Z"
    }
   },
   "id": "5275b5abdb8ae2ad",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(22834380800, 42314694656)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:00.659527Z",
     "start_time": "2024-06-18T10:11:53.469485Z"
    }
   },
   "id": "613910db4a36a1c3",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. scgpt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "621c66876e1eca2a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ntoken': 60697, 'd_model': 512, 'nhead': 8, 'd_hid': 512, 'nlayers': 12, 'nlayers_cls': 3, 'n_cls': 1, 'dropout': 0.2, 'pad_token': '<pad>', 'do_mvc': False, 'do_dab': False, 'use_batch_labels': False, 'num_batch_labels': None, 'domain_spec_batchnorm': False, 'input_emb_style': 'continuous', 'cell_emb_style': 'cls', 'mvc_decoder_style': 'inner product', 'ecs_threshold': 0.3, 'explicit_zero_prob': False, 'fast_transformer_backend': 'flash', 'pre_norm': False, 'vocab': GeneVocab(), 'pad_value': -2, 'n_input_bins': 51, 'use_fast_transformer': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "2024-07-29 16:55:59-load_llm[line-50]-INFO: Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "2024-07-29 16:55:59-load_scgpt[line-113]-INFO: start to get gene embedding!\n",
      "2024-07-29 16:55:59-load_scgpt[line-118]-INFO: finished get gene embedding!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Munch({'model_used': 'scgpt', 'emb_type': 'gene', 'model_file': '../../case/models/scgpt/best_model.pt', 'model_param_file': '../../case/models/scgpt/args.json', 'vocab_file': '../../case/models/scgpt/vocab.json', 'output_dir': '../../case/result/zero-shot/', 'pad_value': -2, 'mask_ratio': 0, 'device': 'cuda', 'CLS': False, 'ADV': False, 'CCE': False, 'MVC': False, 'ECS': False, 'weight_bias_track': False, 'project_name': 'biollm', 'exp_name': 'scgpt_gene_emb_gpu', 'data_source': '/scratch/ssd004/datasets/cellxgene/scb_strict/human', 'save_dir': '/scratch/ssd004/datasets/cellxgene/save/cellxgene_census_human-May23-08-36-2023', 'load_model': None, 'n_hvg': None, 'valid_size_or_ratio': 0.003, 'dist_backend': 'nccl', 'grad_accu_steps': 1, 'pad_token': '<pad>', 'input_style': 'binned', 'input_emb_style': 'continuous', 'n_bins': 51, 'max_seq_len': 1200, 'training_tasks': 'both', 'dist_url': 'tcp://gpu188.cluster.local:53833', 'trunc_by_sample': True, 'vocab_path': '/scratch/ssd004/datasets/cellxgene/scFormer/scformer/tokenizer/default_census_vocab.json', 'rank': 0, 'batch_size': 32, 'eval_batch_size': 64, 'epochs': 6, 'lr': 0.0001, 'scheduler_interval': 100, 'scheduler_factor': 0.99, 'warmup_ratio_or_step': 10000.0, 'no_cls': True, 'no_cce': True, 'fp16': True, 'fast_transformer': True, 'nlayers': 12, 'nheads': 8, 'embsize': 512, 'd_hid': 512, 'dropout': 0.2, 'n_layers_cls': 3, 'log_interval': 9000, 'save_interval': 27000, 'mask_value': -1, 'USE_CLS': False, 'USE_CCE': False, 'USE_GENERATIVE_TRAINING': True, 'world_size': 16, 'distributed': True, 'local_rank': 0, 'gpu': 0})\n",
      "embedding shape: (60697, 512)\n"
     ]
    }
   ],
   "source": [
    "from biollm.utils.utils import load_config\n",
    "import numpy as np\n",
    "from biollm.base.load_scgpt import LoadScgpt\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "\n",
    "config_file = './configs/scgpt_gene_emb.toml'\n",
    "configs = load_config(config_file)\n",
    "\n",
    "obj = LoadScgpt(configs)\n",
    "print(obj.args)\n",
    "\n",
    "gene_ids = list(obj.get_gene2idx().values())\n",
    "gene_ids = np.array(gene_ids)\n",
    "gene_ids = torch.tensor(gene_ids, dtype=torch.long).to(configs.device)\n",
    "obj.model = obj.model.to(configs.device)\n",
    "emb = obj.get_embedding(obj.args.emb_type, gene_ids=gene_ids)\n",
    "print('embedding shape:', emb.shape)\n",
    "if not os.path.exists(configs.output_dir):\n",
    "    os.makedirs(configs.output_dir, exist_ok=True)\n",
    "with open(obj.args.output_dir + f'/scgpt_{obj.args.emb_type}_emb.pk', 'wb') as w:\n",
    "    res = {'gene_names': list(obj.get_gene2idx().keys()), 'gene_emb': emb}\n",
    "    pkl.dump(res, w)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T08:55:59.733967Z",
     "start_time": "2024-07-29T08:55:57.877042Z"
    }
   },
   "id": "d1019236eda70525",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. geneformer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab2f40b631f1bdd1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Munch({'model_used': 'geneformer', 'model_type': 'Pretrained', 'model_file': '/home/share/huadjyin/home/s_huluni/project/bio_model_240614/models/geneformer', 'vocab_file': '/home/share/huadjyin/home/s_huluni/project/bio_model_240614/models/geneformer/gene_vocab.json', 'n_bins': 100, 'input_file': '/home/share/huadjyin/home/s_huluni/project/bio_model/dataset/Zheng68K.h5ad', 'output_dir': '../../case/result/zero-shot/', 'device': 'cuda', 'emb_type': 'gene', 'encoder_dims': 768, 'encoder_heads': 12, 'encoder_depth': 12, 'max_seq_len': 19264, 'use_g2v': False, 'cell_type_key': 'celltype', 'weight_bias_track': False, 'project_name': 'biollm', 'exp_name': 'geneformer_gene_emb_gpu'})\n",
      "embedding shape: torch.Size([25426, 256])\n"
     ]
    }
   ],
   "source": [
    "from biollm.utils.utils import load_config\n",
    "import numpy as np\n",
    "from biollm.base.load_geneformer import LoadGeneformer\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "\n",
    "config_file = './configs/geneformer_gene_emb.toml'\n",
    "configs = load_config(config_file)\n",
    "\n",
    "obj = LoadGeneformer(configs)\n",
    "print(obj.args)\n",
    "\n",
    "gene_ids = list(obj.get_gene2idx().values())\n",
    "gene_ids = np.array(gene_ids)\n",
    "gene_ids = torch.tensor(gene_ids, dtype=torch.long).to(configs.device)\n",
    "obj.model = obj.model.to(configs.device)\n",
    "emb = obj.get_embedding(obj.args.emb_type, gene_ids=gene_ids)\n",
    "print('embedding shape:', emb.shape)\n",
    "if not os.path.exists(configs.output_dir):\n",
    "    os.makedirs(configs.output_dir, exist_ok=True)\n",
    "with open(obj.args.output_dir + f'/geneformer_{obj.args.emb_type}_emb.pk', 'wb') as w:\n",
    "    res = {'gene_names': list(obj.get_gene2idx().keys()), 'gene_emb': emb}\n",
    "    pkl.dump(res, w)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T08:56:05.235837Z",
     "start_time": "2024-07-29T08:56:04.745751Z"
    }
   },
   "id": "7baaa574c86f9b43",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "25424"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.get_gene2idx()['ENSGR0000214717']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T02:47:38.956827Z",
     "start_time": "2024-06-19T02:47:38.936712Z"
    }
   },
   "id": "be5bc42947f5b2e0",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([25424, 25423, 25421,  ...,  1773, 13918, 20751], device='cuda:0')"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T02:44:42.722537Z",
     "start_time": "2024-06-19T02:44:42.678427Z"
    }
   },
   "id": "5eadad6639d1a8e8",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([25426, 256])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.model.bert.embeddings.word_embeddings(gene_ids).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T02:45:33.689295Z",
     "start_time": "2024-06-19T02:45:33.682864Z"
    }
   },
   "id": "ae82d35cf401f302",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. scfoundation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "533feb5dc4eb958"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mask_gene_name': False, 'gene_num': 19266, 'seq_len': 19266, 'encoder': {'hidden_dim': 768, 'depth': 12, 'heads': 12, 'dim_head': 64, 'seq_len': 19266, 'module_type': 'transformer', 'norm_first': False}, 'decoder': {'hidden_dim': 512, 'depth': 6, 'heads': 8, 'dim_head': 64, 'module_type': 'performer', 'seq_len': 19266, 'norm_first': False}, 'n_class': 104, 'pad_token_id': 103, 'mask_token_id': 102, 'bin_num': 100, 'bin_alpha': 1.0, 'rawcount': True, 'model': 'mae_autobin', 'test_valid_train_idx_dict': '/nfs_beijing/minsheng/data/os10000w-new/global_shuffle/meta.csv.train_set_idx_dict.pt', 'valid_data_path': '/nfs_beijing/minsheng/data/valid_count_10w.npz', 'num_tokens': 13, 'train_data_path': None, 'isPanA': False, 'isPlanA1': False, 'max_files_to_load': 5, 'bin_type': 'auto_bin', 'value_mask_prob': 0.3, 'zero_mask_prob': 0.03, 'replace_prob': 0.8, 'random_token_prob': 0.1, 'mask_ignore_token_ids': [0], 'decoder_add_zero': True, 'mae_encoder_max_seq_len': 15000, 'isPlanA': False, 'mask_prob': 0.3, 'model_type': 'mae_autobin', 'pos_embed': False, 'device': 'cuda'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:56:14-load_scfoundation[line-70]-INFO: start to get gene embedding!\n",
      "2024-07-29 16:56:14-load_scfoundation[line-75]-INFO: start to get gene embedding!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Munch({'model_used': 'scfoundation', 'model_file': '../../case/models/scfoundation/models.ckpt', 'vocab_file': '../../case/models/scfoundation/gene_vocab.json', 'key': 'cell', 'emb_type': 'gene', 'input_type': 'singlecell', 'tgthighres': 't4', 'pool_type': 'all', 'batch_size': 64, 'n_bins': 100, 'input_file': '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/downstream_tasks/test/grn/data/Immune_ALL_human.h5ad', 'output_dir': '../../case/result/zero-shot/', 'device': 'cuda', 'quantile_cutoff': 0, 'encoder_dims': 128, 'encoder_heads': 12, 'encoder_depth': 12, 'data_is_raw': False, 'max_seq_len': 19264, 'use_g2v': False, 'hvg_number': 3000, 'weight_bias_track': False, 'project_name': 'biollm', 'exp_name': 'scf_gene_emb_gpu'})\n",
      "embedding shape: (19264, 768)\n"
     ]
    }
   ],
   "source": [
    "from biollm.utils.utils import load_config\n",
    "import numpy as np\n",
    "from biollm.base.load_scfoundation import LoadScfoundation\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "\n",
    "config_file = './configs/scfoundation_gene_emb.toml'\n",
    "configs = load_config(config_file)\n",
    "\n",
    "obj = LoadScfoundation(configs)\n",
    "print(obj.args)\n",
    "\n",
    "gene_ids = list(obj.get_gene2idx().values())\n",
    "gene_ids = np.array(gene_ids)\n",
    "gene_ids = torch.tensor(gene_ids, dtype=torch.long).to(configs.device)\n",
    "obj.model = obj.model.to(configs.device)\n",
    "emb = obj.get_embedding(configs.emb_type, gene_ids=gene_ids)\n",
    "print('embedding shape:', emb.shape)\n",
    "if not os.path.exists(configs.output_dir):\n",
    "    os.makedirs(configs.output_dir, exist_ok=True)\n",
    "with open(obj.args.output_dir + f'/scfoundation_{obj.args.emb_type}_emb.pk', 'wb') as w:\n",
    "    res = {'gene_names': list(obj.get_gene2idx().keys()), 'gene_emb': emb}\n",
    "    pkl.dump(res, w)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T08:56:14.634368Z",
     "start_time": "2024-07-29T08:56:11.564242Z"
    }
   },
   "id": "625e93a89a5ebab5",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19264/19264 [00:00<00:00, 1406497.68it/s]\n",
      "100%|| 185624528/185624528 [03:34<00:00, 865913.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m gene_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(obj\u001B[38;5;241m.\u001B[39mget_gene2idx()\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[1;32m      3\u001B[0m genes_embedding \u001B[38;5;241m=\u001B[39m {gene_names[i]: emb[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(gene_ids))}\n\u001B[0;32m----> 4\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mevalue_gene_emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenes_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgene_pw_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m df\n",
      "Cell \u001B[0;32mIn[3], line 15\u001B[0m, in \u001B[0;36mevalue_gene_emb\u001B[0;34m(emb, gene_pw_dict)\u001B[0m\n\u001B[1;32m     13\u001B[0m threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.1\u001B[39m \u001B[38;5;241m*\u001B[39m i\n\u001B[1;32m     14\u001B[0m g \u001B[38;5;241m=\u001B[39m grn_embed\u001B[38;5;241m.\u001B[39mgenerate_network(threshold\u001B[38;5;241m=\u001B[39mthreshold)\n\u001B[0;32m---> 15\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgene_pw_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthreshold\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(threshold)\n\u001B[1;32m     17\u001B[0m result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModularity\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModularity\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/workspace/omics_model/bio_model/biollm/biollm/evaluate/bm_metrices_grn.py:49\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(G, gene_pw_dict, modularity, biological)\u001B[0m\n\u001B[1;32m     46\u001B[0m modularity_coef \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     47\u001B[0m average_jaccard_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 49\u001B[0m gene_module \u001B[38;5;241m=\u001B[39m \u001B[43mnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommunity\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlouvain_communities\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(gene_module) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m modularity:\n",
      "File \u001B[0;32m~/app/miniconda3/envs/scgpt/lib/python3.9/site-packages/networkx/utils/decorators.py:770\u001B[0m, in \u001B[0;36margmap.__call__.<locals>.func\u001B[0;34m(_argmap__wrapper, *args, **kwargs)\u001B[0m\n\u001B[1;32m    769\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(\u001B[38;5;241m*\u001B[39margs, __wrapper\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 770\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43margmap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lazy_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m__wrapper\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<class 'networkx.utils.decorators.argmap'> compilation 4:4\u001B[0m, in \u001B[0;36margmap_louvain_communities_1\u001B[0;34m(G, weight, resolution, threshold, seed, backend, **backend_kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgzip\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n",
      "File \u001B[0;32m~/app/miniconda3/envs/scgpt/lib/python3.9/site-packages/networkx/utils/backends.py:412\u001B[0m, in \u001B[0;36m_dispatch.__call__\u001B[0;34m(self, backend, *args, **kwargs)\u001B[0m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m/\u001B[39m, \u001B[38;5;241m*\u001B[39margs, backend\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m backends:\n\u001B[1;32m    411\u001B[0m         \u001B[38;5;66;03m# Fast path if no backends are installed\u001B[39;00m\n\u001B[0;32m--> 412\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morig_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    414\u001B[0m     \u001B[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001B[39;00m\n\u001B[1;32m    415\u001B[0m     backend_name \u001B[38;5;241m=\u001B[39m backend\n",
      "File \u001B[0;32m~/app/miniconda3/envs/scgpt/lib/python3.9/site-packages/networkx/algorithms/community/louvain.py:119\u001B[0m, in \u001B[0;36mlouvain_communities\u001B[0;34m(G, weight, resolution, threshold, seed)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Find the best partition of a graph using the Louvain Community Detection\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03mAlgorithm.\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;124;03mlouvain_partitions\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    118\u001B[0m d \u001B[38;5;241m=\u001B[39m louvain_partitions(G, weight, resolution, threshold, seed)\n\u001B[0;32m--> 119\u001B[0m q \u001B[38;5;241m=\u001B[39m \u001B[43mdeque\u001B[49m\u001B[43m(\u001B[49m\u001B[43md\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaxlen\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m q\u001B[38;5;241m.\u001B[39mpop()\n",
      "File \u001B[0;32m~/app/miniconda3/envs/scgpt/lib/python3.9/site-packages/networkx/algorithms/community/louvain.py:197\u001B[0m, in \u001B[0;36mlouvain_partitions\u001B[0;34m(G, weight, resolution, threshold, seed)\u001B[0m\n\u001B[1;32m    194\u001B[0m     graph\u001B[38;5;241m.\u001B[39madd_weighted_edges_from(G\u001B[38;5;241m.\u001B[39medges(data\u001B[38;5;241m=\u001B[39mweight, default\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    196\u001B[0m m \u001B[38;5;241m=\u001B[39m graph\u001B[38;5;241m.\u001B[39msize(weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 197\u001B[0m partition, inner_partition, improvement \u001B[38;5;241m=\u001B[39m \u001B[43m_one_level\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartition\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresolution\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_directed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m improvement \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m improvement:\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;66;03m# gh-5901 protect the sets in the yielded list from further manipulation here\u001B[39;00m\n",
      "File \u001B[0;32m~/app/miniconda3/envs/scgpt/lib/python3.9/site-packages/networkx/algorithms/community/louvain.py:266\u001B[0m, in \u001B[0;36m_one_level\u001B[0;34m(G, m, partition, resolution, is_directed, seed)\u001B[0m\n\u001B[1;32m    264\u001B[0m best_mod \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    265\u001B[0m best_com \u001B[38;5;241m=\u001B[39m node2com[u]\n\u001B[0;32m--> 266\u001B[0m weights2com \u001B[38;5;241m=\u001B[39m \u001B[43m_neighbor_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbrs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mu\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode2com\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_directed:\n\u001B[1;32m    268\u001B[0m     in_degree \u001B[38;5;241m=\u001B[39m in_degrees[u]\n",
      "File \u001B[0;32m~/app/miniconda3/envs/scgpt/lib/python3.9/site-packages/networkx/algorithms/community/louvain.py:337\u001B[0m, in \u001B[0;36m_neighbor_weights\u001B[0;34m(nbrs, node2com)\u001B[0m\n\u001B[1;32m    335\u001B[0m weights \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mfloat\u001B[39m)\n\u001B[1;32m    336\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m nbr, wt \u001B[38;5;129;01min\u001B[39;00m nbrs\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 337\u001B[0m     weights[node2com[nbr]] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m wt\n\u001B[1;32m    338\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m weights\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "emb = emb.detach().cpu().numpy()\n",
    "gene_names = list(obj.get_gene2idx().keys())\n",
    "genes_embedding = {gene_names[i]: emb[i] for i in range(len(gene_ids))}\n",
    "df = evalue_gene_emb(genes_embedding, gene_pw_dict)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T03:02:28.294349Z",
     "start_time": "2024-05-16T02:33:57.188387Z"
    }
   },
   "id": "10a932484af6ccdd",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
